---
title: "Surviving Survival Analysis"
author: "Claudia Liauw"
date: "2026-01-13"
categories: [data science]
---

Survival analysis is a lesser known but important statistical approach commonly used in clinical settings to predict patient survival time. Besides predicting literal survival, it also has applications in engineering, with time to machine failure, or business, with time to customer churn. You might be starting to see a pattern here - the variable of interest in survival analysis is the time until an event occurs. This could even be a composite event. In cancer research, clinicians are commonly interested in recurrence-free survival, the time that a patient both survives and is recurrence-free. This means we measure the time taken to cancer recurrence or death, whichever comes first.

# Censoring

You may be wondering, since the time until an event occurs is a continuous variable, why not just use linear regression? The answer is due to data censoring, where the time to event is unknown for some samples. This occurs when the event has not occurred or only occurred after the end of the study (@fig-censor). It can also occur if the patient is lost to follow-up during the study. In linear regression, we will have to discard these samples, since they do not have values for our variable of interest. That will not be a good approach as we will lose statistical power and it also results in a biased sample. For example, if we only use data from those who have experienced a cancer recurrence or died, then we are biasing the sample towards patients with poorer outcomes. In these cases of censoring, we know that the time to event is greater than the time to censoring and survival analysis allows that information to be incorporated.

![Right censoring due to event occurring after the end of the study or lost to follow-up](censor.jpg){#fig-censor width="500"}

# Key concepts in survival analysis

The aim in survival analysis is to model the survival function. The survival function is a function of time and gives the probability that an event will not occur before a certain time. This is actually just the complement of the cumulative distribution function (CDF), which gives the probability that an event will occur before a certain time! The survival function is a monotonically decreasing graph, as we expect the probability of survival to decrease over time (@fig-weibull). 

The hazard function is another important concept in survival analysis that forms the basis of regression models. It is the instantaneous rate of event occurrence per unit time. In other words, given that the event has not occurred, what is the probability of the event occurring now?

We will use the Weibull distribution to illustrate these concepts. It is commonly used to model the survival function. You may recall the Weibull distribution as a generalisation of the exponential distribution, characterised by shape and scale parameters. As explained above, the survival function can be obtained from Weibull's CDF, and the hazard function can be obtained from the survival function. This post serves to provide a conceptual understanding, so we will not go into the mathematical details. The shape and scale parameters change how the survival and hazard functions look like (@fig-weibull). In this case, we can see that at 1 year, type A has a higher survival probability than type B.

![Survival and hazard functions of Weibull distributions with different parameters](weibull.png){#fig-weibull}

# Survival analysis models

In regression analysis, we are often concerned with how explanatory variables are associated with the response variable. However, the survival function only tells us the probability of survival, not how survival changes with explanatory variables, so we will need to use a different modelling approach. 

The most common survival analysis model is the Cox proportional hazards model (CoxPH). Essentially, we model the hazard function $h_i(t)$ for every sample $i$:

 $h_i(t) = h_0(t)\exp(b_1x_1 + b_2x_2 + \dots + b_px_p)$. 

Like with linear regression, we have coefficients $b$ and values of explanatory variables $x$. However, instead of an intercept, we have a baseline hazard $h_0(t)$ and each sample has a risk score$\exp(b_1x_1 + b_2x_2 + \dots + b_px_p)$ that is multiplied with the baseline hazard to give the hazard for the individual sample. Based on this, the hazard of every individual is a multiple of the baseline hazard, hence the name "proportional hazards". To contextualise this, let's say $x_1$ refers to whether a patient smokes, which we know to increase the risk of death. When fitting the coefficients, we will likely get a positive $b_1$, so a patient who smokes will have a higher risk than a patient who does not. This may correspond to a hazard function of type B and A in the graph above, for example. Translating to the survival function, we see that type A (the patient who does not smoke) has a higher probability of survival than type B (the patient who smokes) at time points we may be interested in.

Besides CoxPH, there are also machine learning models for survival analysis. Random forest is a standard machine learning model which uses an ensemble of decision trees that split features in a way that maximises the difference between child nodes (@fig-rf). Random survival forest extends random forest to survival analysis. In this case, the difference that is maximised is the log-rank statistic. Briefly speaking, the log-rank statistic measures the differences in survival distributions between two samples by comparing the number of observed and expected (if there were no differences between the two groups) events over time.

![Decision tree in a random forest splitting parent into child nodes based on feature value threshold](rf.jpg){#fig-rf width="300"}

# Evaluation

Survival analysis models are evaluated on their discrimination and calibration. Discrimination refers to whether the model can distinguish between high and low risk patients. The most common metric used is the concordance index (C-index). This measures whether subjects who fail earlier have a higher risk score as mentioned above. This is a single value over the entire study period. If a specific time range is of primary interest (e.g. is the model good at predicting death within 2 years?), a more useful discrimination metric will be the time-dependent area under the curve (tdAUC). This measures whether subjects who fail by a given time have a higher risk score than subjects who fail after this time. Calibration measures whether the model can accurately predict survival probability. This is assessed through comparing predicted probability versus fraction of actual events. For example, among those with a predicted 0.2 probability of death, we expect 20% will die if the model is well-calibrated.

# Conclusion

To sum up, survival analysis is a powerful tool in your data science toolkit when dealing with censored data. Now you know how to construct survival analysis models and evaluate them!

# References

Bradburn, M., Clark, T., Love, S. *et al.* Survival Analysis Part II: Multivariate data analysis – an introduction to concepts and methods. *Br J Cancer* **89**, 431–436 (2003). https://doi.org/10.1038/sj.bjc.6601119

Clark, T., Bradburn, M., Love, S. *et al.* Survival Analysis Part I: Basic concepts and first analyses. *Br J Cancer* **89**, 232–238 (2003). https://doi.org/10.1038/sj.bjc.6601118

Kent B. Applications of survival analysis (that aren’t clinical research) – Brian Patrick Kent [Internet]. Brian Patrick Kent. 2021 [cited 2026 Jan 13]. Available from: https://www.crosstab.io/articles/survival-analysis-applications/

Rodríguez-Arelis A, Nickchi P, Lourenzutti R, Coia V. Lecture 5: Survival Analysis [Internet]. Available from: https://pages.github.ubc.ca/mds-2025-26/DSCI_562_regr-2_students/notes/lecture5_survival_analysis.html
‌
